# Movie Search Engine


free form search on plot overviews
## Dataset:
https://www.kaggle.com/rounakbanik/the-movies-dataset


## Dependencies:
csv, nltk, numpy, json, flask, pandas <br/>
Python 3.7

## app url
http://emmanuelrocha001.pythonanywhere.com/

## How to run locally:
python app.py <br/> <br/>
runs on localhost:5000
 

## Dataset pre-processing
### Tokenization
  __[1]:__ change casing to lowercase <br/>
  __[2]:__ remove punctuation <br/>
  __[3]:__ filter out stop-words <br/>

### Posting Lists

dataset tokens are processed and a json file is generated with the following data<br/><br/>
__term__: unique term in the dataset<br/>
__frequency__: number of times the term appears in the dataset<br/>
__posting list__: documents in which the term appears<br/><br/>

| term         | frequency      |  posting list |
| :---         |     :---:      |          ---: |
| led          | 484            | \[0, 218, 372, 443...\]  |

## Document Ranking

### Term frequency
__TF(t):__ ( Number of times term t appears in a document ) / ( Total number of terms in the document )
### Inverse Document Frequency
__IDF(t):__ log ( Total number of documents / Number of documents with term t in it ).
### Term frequency-Inverse document frequency
__TF-IDF(t):__ tf(t) x idf(t)
### Overall Document Score
__Score:__  sum of the tf-idf for each term in the query

## Challenges
The size of the dataset, around 40k documents, was considerably larger than anything I had worked with in the past. My first instinct was to utilize javascript for both pre-processing and document score calculation, but given the size of the data this quickly proved to be not feasible. As recommended, I switched over to Flask. By Utilizing both Pandas and NLTK generating the posting lists became trivial. Given my new workflow environment, document scoring also did not prove to be too cumbersome to implement. The next challenge faced was figuring out a way to get user input, in this case the search query. The easiest method I found was parametrizing the url. In addition, displaying the information generated proved to be more challenging, as they are not set html files and are generated by the jinja engine using templates. Although the engine provides an environment that is more flexible and reusable, it can quickly become cumbersome to work with and does not always feel intuitive.

## References
__tf-idf__<br/>
https://en.wikipedia.org/wiki/Tf%E2%80%93idf

# Classifier
genre classification on provided text
## Dataset pre-processing

Feature lists are processed from the csv file. The Feature lists along with the Posting lists are use to generate the json file containing the pre-processed data require for classification calculations. Data format is shown below.<br/>

### Feature Lists
__document id__: document index<br/>
__terms__: a list of terms and their corresponding frequencies in the current document<br/>
__genre__: pre-assigned genre<br/><br/>

| Document id  | terms      |  genre |
| :---         |     :---:      |          ---: |
| 0          | { "led": 1, "Buzz": 2, "Woody": 3 }        | Animation  |

### Genres pre-processed data
__vocabulary size__: number of unique terms in the dataset<br/>
__genres__: a list of unique genres, and both the total terms in genre and probability<br/><br/>

| vocabulary size  | genres       |  
| :---         |     :---:      |        
| 73943          | { "Animation": { "total_term_in_genre": 1120 , "probability": 0.026566095021229155 }, ... }        | 

### Terms pre-processed data
__term__: term in vocabulary<br/>
__genres__: a list of genres and corresponding frequencies in which the term appears in<br/><br/>

| term  | frequencies       |  
| :---         |     :---:      |        
| woody          | {"Animation": 6, "Adventure": 0, "Romance": 0, "Comedy": 11...}       | 
